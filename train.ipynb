{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from net.aams import AttentionNet, Encoder, Decoder\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Basic options\n",
    "parser.add_argument('--content_dir', type=str, default='./datasets/content_set/val2014',\n",
    "                    help='Directory path to a batch of content images')\n",
    "parser.add_argument('--style_dir', type=str, default='./datasets/style_set/val2014',\n",
    "                    help='Directory path to a batch of style images')\n",
    "parser.add_argument('--mix_dir', type=str, default='./datasets/content_style_mix_set/val2014')\n",
    "\n",
    "# training options\n",
    "parser.add_argument('--save_dir', default='./models/',\n",
    "                    help='Directory to save the model')\n",
    "parser.add_argument('--log_dir', default='./logs',\n",
    "                    help='Directory to save the log')\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--lr_decay', type=float, default=5e-5)\n",
    "parser.add_argument('--max_iter', type=int, default=80000)\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "parser.add_argument('--n_threads', type=int, default=2)\n",
    "parser.add_argument('--save_model_interval', type=int, default=100)\n",
    "parser.add_argument('--start_iter', type=float, default=5000)\n",
    "parser.add_argument('--seperate', type=bool, default=False)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "_R_MEAN = 123.68\n",
    "_G_MEAN = 116.78\n",
    "_B_MEAN = 103.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size=(512, 512)),\n",
    "        transforms.RandomCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((_R_MEAN/255.0, _G_MEAN/255.0, _B_MEAN/255.0), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=(512, 512)),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((_R_MEAN/255.0, _G_MEAN/255.0, _B_MEAN/255.0), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "class FlatFolderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        super(FlatFolderDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.paths = os.listdir(self.root)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def name(self):\n",
    "        return 'FlatFolderDataset'\n",
    "\n",
    "train_set = FlatFolderDataset(args.content_dir, data_transforms['train'])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=args.batch_size,\n",
    "    num_workers=args.n_threads)\n",
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images = dataiter.next()\n",
    "print(images[0].size())\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionNet(seperate=args.seperate)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encode.parameters():\n",
    "    param.requires_grad = False\n",
    "if args.seperate == True:\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.self_attn_content.parameters()},\n",
    "        {'params': model.self_attn_style.parameters()},\n",
    "        {'params': model.content_decode.parameters()},\n",
    "        {'params': model.style_decode.parameters()},\n",
    "    ], lr=args.lr)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.decode.parameters()},\n",
    "        {'params': model.self_attn.parameters()},\n",
    "    ], lr=args.lr)\n",
    "\n",
    "if(args.start_iter > 0):\n",
    "    optimizer.load_state_dict(torch.load(args.save_dir + 'optimizer_iter_' + str(args.start_iter) + '.pth'))\n",
    "    \n",
    "else:\n",
    "    \n",
    "loss_seq = {'total': [], 'construct': [], 'percept': [], 'tv': [], 'attn': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lastest_arverage_value(values, length=100):\n",
    "    if len(values) < length:\n",
    "        length = len(values)\n",
    "    return sum(values[-length:])/length\n",
    "\n",
    "def adjust_learning_rate(optimizer, iteration_count):\n",
    "    \"\"\"Imitating the original implementation\"\"\"\n",
    "    lr = args.lr / (1.0 + args.lr_decay * iteration_count)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(args.start_iter, args.max_iter)):\n",
    "    adjust_learning_rate(optimizer, iteration_count=i)\n",
    "    if args.seperate == False:\n",
    "        content_images = next(train_iter).to(device)\n",
    "        losses, _, _ = model(content_images)\n",
    "    total_loss = losses['total']\n",
    "    \n",
    "    for name, vals in loss_seq.items():\n",
    "        loss_seq[name].append(losses[name].item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i + 1) % args.save_model_interval == 0 or (i + 1) == args.max_iter:\n",
    "        print(\"%s: Iteration: [%d/%d]\\tRecon Loss: %2.4f\\tPercept Loss: %2.4f\\tTV Loss: %2.4f\\tAttn Loss: %2.4f\\tTotal: %2.4f\"%(time.ctime(),i+1, \n",
    "                args.max_iter, lastest_arverage_value(loss_seq['construct']), lastest_arverage_value(loss_seq['percept']), \n",
    "                lastest_arverage_value(loss_seq['tv']), lastest_arverage_value(loss_seq['attn']), lastest_arverage_value(loss_seq['total'])))\n",
    "        if args.seperate == False:\n",
    "            state_dict = model.decode.state_dict()\n",
    "            for key in state_dict.keys():\n",
    "                state_dict[key] = state_dict[key].to(torch.device('cpu'))\n",
    "            torch.save(state_dict,\n",
    "                       '{:s}/decoder_iter_{:d}.pth'.format(args.save_dir,\n",
    "                                                               i + 1))\n",
    "            state_dict = model.self_attn.state_dict()\n",
    "            for key in state_dict.keys():\n",
    "                state_dict[key] = state_dict[key].to(torch.device('cpu'))\n",
    "            torch.save(state_dict,\n",
    "                       '{:s}/attention_kernel_iter_{:d}.pth'.format(args.save_dir,\n",
    "                                                               i + 1))\n",
    "            state_dict = optimizer.state_dict()\n",
    "            torch.save(state_dict,\n",
    "                       '{:s}/optimizer_iter_{:d}.pth'.format(args.save_dir,\n",
    "                                                               i + 1))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
