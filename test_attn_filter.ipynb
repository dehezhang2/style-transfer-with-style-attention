{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_swap(content_feature, style_feature, kernel_size=5, stride=1):\n",
    "    # content_feature and style_feature should have shape as (1, C, H, W)\n",
    "    # kernel_size here is equivalent to extracted patch size\n",
    "\n",
    "    # extract patches from style_feature with shape (1, C, H, W)\n",
    "    kh, kw = kernel_size, kernel_size\n",
    "    sh, sw = stride, stride\n",
    "\n",
    "    patches = style_feature.unfold(2, kh, sh).unfold(3, kw, sw)\n",
    "    \n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5)\n",
    "    \n",
    "    patches = patches.reshape(-1, *patches.shape[-3:]) # (patch_numbers, C, kh, kw)\n",
    "    print(patches.shape)\n",
    "    # calculate Frobenius norm and normalize the patches at each filter\n",
    "    norm = torch.norm(patches.reshape(patches.shape[0], -1), dim=1).reshape(-1, 1, 1, 1)\n",
    "    \n",
    "    noramalized_patches = patches / norm\n",
    "\n",
    "    conv_out = F.conv2d(content_feature, noramalized_patches)\n",
    "    \n",
    "    # calculate the argmax at each spatial location, which means at each (kh, kw),\n",
    "    # there should exist a filter which provides the biggest value of the output\n",
    "    one_hots = torch.zeros_like(conv_out)\n",
    "    one_hots.scatter_(1, conv_out.argmax(dim=1, keepdim=True), 1)\n",
    "\n",
    "    # deconv/transpose conv\n",
    "    deconv_out = F.conv_transpose2d(one_hots, patches)\n",
    "\n",
    "    # calculate the overlap from deconv/transpose conv\n",
    "    overlap = F.conv_transpose2d(one_hots, torch.ones_like(patches))\n",
    "\n",
    "    # average the deconv result\n",
    "    res = deconv_out / overlap\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((1, 512, 32, 32))\n",
    "b = torch.rand((1, 512, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 512, 5, 5])\n",
      "torch.Size([1, 512, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "res = style_swap(b, a)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_scale_style_swap(content_feature, style_feature, kernel_size=5, stride=1):\n",
    "    c_shape = content_feature.shape\n",
    "    s_shape = style_feature.shape\n",
    "    assert (c_shape[1] == s_shape[1])\n",
    "    \n",
    "    combined_feature_maps = []\n",
    "    for beta in [1.0/2, 1.0/(2**0.5), 1.0]:\n",
    "        new_height = int(float(s_shape[2]) * beta)\n",
    "        new_width = int(float(s_shape[3]) * beta)\n",
    "        tmp_style_features = F.interpolate(style_feature, \\\n",
    "            size=(new_height, new_width), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        combined_feature = style_swap(content_feature, \\\n",
    "            style_feature, kernel_size=kernel_size, stride=stride)\n",
    "        \n",
    "        combined_feature_maps.append(combined_feature)\n",
    "    combined_feature_maps.append(content_feature)\n",
    "    return combined_feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = multi_scale_style_swap(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 32, 32])\n",
      "torch.Size([1, 512, 32, 32])\n",
      "torch.Size([1, 512, 32, 32])\n",
      "torch.Size([1, 512, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for feature in res:\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def KMeans(image, clusters_num):\n",
    "# # KMeans for only h*w*1 tensors\n",
    "#     image = image.squeeze()\n",
    "#     _points = image.reshape(-1, 1)\n",
    "#     # randomly permuate the points and select cluster_num centroids\n",
    "#     idx = torch.randperm(_points.shape[0])\n",
    "#     centroids = _points[idx][:clusters_num]\n",
    "    \n",
    "#     # expand the points dimension to the cluster\n",
    "#     points_expanded = _points.repeat(1, clusters_num)\n",
    "    \n",
    "#     for i in range(80):\n",
    "#         centroids_expanded  = centroids.permute(1,0).repeat(points_expanded.shape[0], 1)\n",
    "#         distances = (points_expanded - centroids_expanded) ** 2\n",
    "#         # same shape as distance, by expanding argmin\n",
    "#         distances_min_expand = distances.min(1).values.unsqueeze(1).repeat(1, distances.shape[1])\n",
    "#         # the mask that can be used to filter the value\n",
    "#         mask = (distances == distances_min_expand).float()\n",
    "#         centroids = (distances_min_expand * mask).sum(0).unsqueeze(1)\n",
    "#         print(centroids)\n",
    "        \n",
    "#     return centroids\n",
    "\n",
    "# https://www.kernel-operations.io/keops/_auto_tutorials/kmeans/plot_kmeans_torch.html\n",
    "def KMeans(x, K=10, Niter=80, verbose=True):\n",
    "    N, D = x.shape  # Number of samples, dimension of the ambient space\n",
    "\n",
    "    # K-means loop:\n",
    "    # - x  is the point cloud,\n",
    "    # - cl is the vector of class labels\n",
    "    # - c  is the cloud of cluster centroids\n",
    "\n",
    "    c = x[:K, :].clone()  # Simplistic random initialization\n",
    "    x_i = x.unsqueeze(1)  # (Npoints, 1, D)\n",
    "    for i in range(Niter):\n",
    "        c_j = c.unsqueeze(0)  # (1, Nclusters, D)\n",
    "        D_ij = ((x_i - c_j) ** 2).sum(-1)  # (Npoints, Nclusters) symbolic matrix of squared distances\n",
    "        cl = D_ij.argmin(dim=1).long().view(-1)  # Points -> Nearest cluster\n",
    "\n",
    "        Ncl = torch.bincount(cl).type(torch.float64)  # Class weights\n",
    "        for d in range(D):  # Compute the cluster centroids with torch.bincount:\n",
    "            c[:, d] = torch.bincount(cl, weights=x[:, d]) / Ncl\n",
    "            \n",
    "    return cl, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_stroke_fusion(stylized_maps, attention_map, theta=50.0, mode='softmax'):\n",
    "    stroke_num = len(stylized_maps)\n",
    "    if stroke_num == 1:\n",
    "        return stylized_maps[0]\n",
    "    \n",
    "    one_channel_attention = torch.mean(attention_map, 1).unsqueeze(1)\n",
    "    origin_shape = one_channel_attention.shape\n",
    "    one_channel_attention = one_channel_attention.reshape((-1, 1)) # stretch to tensor (hw)* 1 \n",
    "    _, centroids = KMeans(one_channel_attention, stroke_num)\n",
    "    \n",
    "    one_channel_attention = one_channel_attention.reshape(origin_shape)\n",
    "    \n",
    "    saliency_distances = []\n",
    "    for i in range(stroke_num):\n",
    "        saliency_distances.append(torch.abs(one_channel_attention - centroids[i]))\n",
    "    \n",
    "    multi_channel_saliency = torch.cat(saliency_distances, 1)\n",
    "    \n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    multi_channel_saliency = softmax(theta*(1.0 - multi_channel_saliency))\n",
    "    \n",
    "    finial_stylized_map = 0\n",
    "    for i in range(stroke_num):\n",
    "        temp = multi_channel_saliency[0, i, :, :].unsqueeze(0).unsqueeze(0)\n",
    "        finial_stylized_map += temp * stylized_maps[i]\n",
    "    return finial_stylized_map, centroids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((1, 512, 32, 32))\n",
    "b = torch.rand((1, 512, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "finial_stylized_map, _ = multi_stroke_fusion([a, a, a], b)\n",
    "print(finial_stylized_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zca_normalization(features):\n",
    "    # [b, c, h, w]\n",
    "    shape = features.shape\n",
    "\n",
    "    # reshape the features to orderless feature vectors\n",
    "    mean_features = torch.mean(features, dim=(2, 3), keepdims=True)\n",
    "    unbiased_features = (features - mean_features).view(shape[0], shape[1], -1) # [b, c, h*w]\n",
    "\n",
    "    # get the convariance matrix\n",
    "    gram = torch.bmm(unbiased_features, unbiased_features.permute(0, 2, 1)) # [b, c, c]\n",
    "    gram = gram / (shape[1] * shape[2] * shape[3])\n",
    "\n",
    "    # converting the feature spaces\n",
    "    u, s, v = torch.svd(gram, compute_uv=True)\n",
    "    # u: [b, c, c], s: [b, c], v: [b, c, c]\n",
    "    s = torch.unsqueeze(s, dim=1)\n",
    "\n",
    "    # get the effective singular values\n",
    "    valid_index = (s > 0.00001).float()\n",
    "    s_effective = torch.max(s, torch.empty(s.shape).fill_(0.00001))\n",
    "    sqrt_s_effective = torch.sqrt(s_effective) * valid_index\n",
    "    sqrt_inv_s_effective = torch.sqrt(1.0 / s_effective) * valid_index\n",
    "    print(s_effective.shape)\n",
    "\n",
    "    # colorization functions\n",
    "    colorization_kernel = torch.bmm((u * sqrt_inv_s_effective), v.permute(0, 2, 1))\n",
    "\n",
    "    # normalized features\n",
    "    normalized_features = torch.bmm(unbiased_features.permute(0, 2, 1), u).permute(0, 2, 1)\n",
    "    normalized_features = (normalized_features.permute(0, 2, 1) * sqrt_inv_s_effective).permute(0, 2, 1)\n",
    "    normalized_features = torch.bmm(normalized_features.permute(0, 2, 1), v.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "    normalized_features = normalized_features.view(shape)\n",
    "\n",
    "    return normalized_features, colorization_kernel, mean_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zca_colorization(normalized_features, colorization_kernel, mean_features):\n",
    "    # broadcasting the tensors for matrix multiplication\n",
    "    shape = normalized_features.shape\n",
    "    normalized_features = normalized_features.view(shape[0], shape[1], -1) # [b, c, h*w]\n",
    "\n",
    "    colorized_features = torch.bmm(normalized_features.permute(0, 2, 1), colorization_kernel).permute(0, 2, 1)\n",
    "    colorized_features = colorized_features.view(shape) + mean_features\n",
    "\n",
    "\n",
    "    # normalized_features = normalized_features.permute(0, 2, 3, 1).view(shape[0], -1, shape[1]) # [b, c, h*w]\n",
    "\n",
    "    # colorized_features = torch.bmm(normalized_features, colorization_kernel)\n",
    "    # colorized_features = colorized_features.permute(0, 2, 1).view(shape) + mean_features\n",
    "    return colorized_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
